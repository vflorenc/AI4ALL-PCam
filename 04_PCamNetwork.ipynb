{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network on PCam Data\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a perceptron on the pcam data\n",
    "\n",
    "Load the pickle file and create the normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcam = pickle.load(open('content/drive/My Drive/pcam.pkl', 'rb'))\n",
    "train_images, train_y = pcam['train']\n",
    "valid_images, valid_y = pcam['valid']\n",
    "test_images, test_y = pcam['test']\n",
    "\n",
    "transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_X = torch.stack([transform(torch.as_tensor(i/255.0, dtype=torch.float32).permute(2,0,1)) for i in train_images])\n",
    "train_tensor_y = torch.as_tensor(train_y,dtype=torch.long)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor_X, train_tensor_y)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tensor_X = torch.stack([transform(torch.as_tensor(i/255.0, dtype=torch.float32).permute(2,0,1)) for i in valid_images])\n",
    "valid_tensor_y = torch.as_tensor(valid_y,dtype=torch.long)\n",
    "valid_dataset = torch.utils.data.TensorDataset(valid_tensor_X, valid_tensor_y)\n",
    "valloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor_X = torch.stack([transform(torch.as_tensor(i/255.0, dtype=torch.float32).permute(2,0,1)) for i in test_images])\n",
    "test_tensor_y = torch.as_tensor(test_y,dtype=torch.long)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor_X, test_tensor_y)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View 60 of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].permute(1,2,0).numpy()/2 + 0.5)\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model that takes a flattened version of the image and has three linear layers (similar to what we used for MNIST yesterday) and train it.\n",
    "\n",
    "Before you move on, you must achieve .65 or higher on the test set. You may change any parameters or model layers. You must set the learning rate for the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 96*96*3\n",
    "hidden_sizes = [528, 256, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "model = model.to('cuda')\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "### YOUR CODE HERE - find a learning rate that allows the model to achieve above 65% on the test set\n",
    "optimizer = optim.SGD(model.parameters(), lr= , momentum=0.96, weight_decay = 0.001)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1).to('cuda')\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels.to('cuda'))\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i].view(1, -1).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu().numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy on Test Set =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a convolutional network on PCam\n",
    "\n",
    "Below, we will add convolutional layers into our model. These convolutional layers use the same convolution we learned in the \"Linear Filtering\" notebook, but with learned weights for the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 16, 3)\n",
    "        self.fc1 = nn.Linear(16*10*10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 16*10*10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, be sure to achieve .75 or higher on the test set before moving on. Set the learning rate (lr), momentum, and weight_decay parameters. Feel free to change the model as well, but changing the model is not necessary to achieve .75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to('cuda')\n",
    "model.train()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "### Your code here ###\n",
    "optimizer = optim.SGD(model.parameters(), lr=, momentum=, weight_decay = )\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  images = images.to('cuda')\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i][None,:,:,:].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu().numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Once you've finished finding parameters that work for the network, we will try to train the exact same model with ALL of the data. This will take longer, so make sure you are happy with your parameters before you start training on all of the data.\n",
    "\n",
    "## Training on ALL of the PCam data\n",
    "\n",
    "Load the full PCam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_y = pickle.load(open('/content/drive/My Drive/pcam_train.pkl', 'rb'))\n",
    "test_images, test_y = pickle.load(open('/content/drive/My Drive/pcam_test.pkl', 'rb'))\n",
    "transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_X = torch.stack([transform(torch.as_tensor(i/255.0, dtype=torch.float32).permute(2,0,1)) for i in train_images])\n",
    "train_tensor_y = torch.as_tensor(train_y,dtype=torch.long)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor_X, train_tensor_y)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_tensor_X = torch.stack([transform(torch.as_tensor(i/255.0, dtype=torch.float32).permute(2,0,1)) for i in test_images])\n",
    "test_tensor_y = torch.as_tensor(test_y,dtype=torch.long)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor_X, test_tensor_y)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your parameters and run the training on the full dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to('cuda')\n",
    "model.train()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "### Your code here ###\n",
    "optimizer = optim.SGD(model.parameters(), lr=, momentum=, weight_decay = )\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  images = images.to('cuda')\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i][None,:,:,:].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu().numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "1. Why did adding more data help?\n",
    "\n",
    "\n",
    "2. What parameter/model change did you make that helped improve your score the most?\n",
    "\n",
    "\n",
    "3. What features can we capture with neural nets that we didn't capture with average pixel value or histograms?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskrcnn_benchmark",
   "language": "python",
   "name": "maskrcnn_benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
